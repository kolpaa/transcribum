import random
import string
from typing import List, Optional
import chromadb
import ollama
from yandex_cloud_ml_sdk import YCloudML

from src.domain.interfaces import IAIService
from src.domain.constants import LLMPrompts

class AIService(IAIService):
    def __init__(
        self,
        embedding_model: str = "mxbai-embed-large",
        generation_model: str = "deepseek-r1:8b",
	auth: str,
	folder_id: str
    ):
        self.client = chromadb.Client()
        self.embedding_model = embedding_model
        self.generation_model = generation_model
        self.sdk = YCloudML(folder_id=folder_id, auth=auth)
        self.WORDS_PER_CHUNK = 1500

    def make_embedding_collection(self, file_path: str) -> str:
        """–ß–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –∏ —Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤ ChromaDB."""
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                lines = [line.strip() for line in file if line.strip()]
        except FileNotFoundError:
            raise ValueError(f"File not found: {file_path}")

        collection_name = "".join(random.choice(string.ascii_lowercase) for _ in range(10))
        collection = self.client.create_collection(name=collection_name)

        for i, text in enumerate(lines):
            try:
                response = ollama.embeddings(model=self.embedding_model, prompt=text)
                collection.add(
                    ids=[str(i)],
                    embeddings=[response["embedding"]],
                    documents=[text]
                )
            except Exception as e:
                print(f"Error processing line {i}: {e}")
        return collection_name

    def generate_answer(self, prompt: str, collection_name: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–º–ø—Ç–∞ –∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–æ–ª–ª–µ–∫—Ü–∏–∏."""
        collection = self.client.get_collection(name=collection_name)
        if not collection:
            raise ValueError("Collection not initialized. Call prepare_text_for_embedding() first.")

        try:
            # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            embedding_response = ollama.embeddings(
                model=self.embedding_model,
                prompt=prompt
            )
            results = collection.query(
                query_embeddings=[embedding_response["embedding"]],
                n_results=1
            )
            context = results['documents'][0][0]

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            generation_response = ollama.generate(
                think=False,
                model=self.generation_model,
                prompt=f"Context: {context}\n\nQuestion: {prompt}\nAnswer:",
            )
            return generation_response['response']
        except Exception as e:
            raise RuntimeError(f"Error generating answer: {e}")

    def generate_remote_api_answer(self, file_path, prompt):
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read()

        print("üìÑ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏...")
        chunks = self.split_text_by_paragraphs(text)
        print(f"üîπ –ù–∞–π–¥–µ–Ω–æ —á–∞—Å—Ç–µ–π: {len(chunks)}")

        summaries = []
        for i, chunk in enumerate(chunks):
            print(f"üß† –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞—Å—Ç–∏ {i+1}/{len(chunks)}...")
            summary = self.summarize_chunk(self.sdk, chunk, prompt)
            summaries.append(summary)

        print("üß© –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —á–∞—Å—Ç–∏—á–Ω—ã—Ö summary –≤ –∏—Ç–æ–≥–æ–≤–æ–µ...")
        final_summary = self.summarize_all(self.sdk, summaries, prompt)

        print("\n‚úÖ –ì–æ—Ç–æ–≤–æ. –ò—Ç–æ–≥–æ–≤–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ:")
        print("-" * 50)
        print(final_summary)
        return final_summary 




    def cleanup(self, collection_name) -> None:
        """–£–¥–∞–ª—è–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é ChromaDB –∏ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ—Ç —Ä–µ—Å—É—Ä—Å—ã."""
        self.client.delete_collection(collection_name)

    def split_text_by_paragraphs(self, text: str) -> list[str]:
        paragraphs = text.split("\n\n")
        chunks = []
        current = []

        word_count = 0
        for para in paragraphs:
            words = para.split()
            if word_count + len(words) > self.WORDS_PER_CHUNK and current:
                chunks.append("\n\n".join(current))
                current = [para]
                word_count = len(words)
            else:
                current.append(para)
                word_count += len(words)

        if current:
            chunks.append("\n\n".join(current))

        return chunks


    def summarize_chunk(self, sdk: YCloudML, chunk: str, prompt) -> str:
        messages = [
            {"role": "system", "text": "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º."},
            {"role": "user", "text": f"{prompt}:\n\n{chunk}"}
        ]
        result = (
            sdk.models.completions("yandexgpt").configure(temperature=0.5).run(messages)
        )
        print(result.alternatives[0].text)
        return result.alternatives[0].text


    def summarize_all(self, sdk: YCloudML, partial_summaries: list[str], prompt) -> str:
        merged = "\n\n".join(partial_summaries)
        if prompt == LLMPrompts.MAKE_POST:
            messages = [
                {"role": "system", "text": "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π –ø–æ—Å—Ç –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π —Å–µ—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–µ–π."},
                {"role": "user", "text": (
                    "–í–æ—Ç –∫—Ä–∞—Ç–∫–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞—Å—Ç–µ–π –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n\n"
                    f"{merged}\n\n"
                    "–°–æ—Å—Ç–∞–≤—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –ø–æ—Å—Ç –¥–ª—è —Å–æ—Ü–∏–∞–ª—å–Ω–æ–π —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—è –∏—Ö"
                )}
            ]
        elif prompt == LLMPrompts.MAKE_SUMMARY:
            messages = [
                {"role": "system", "text": "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π –∏—Ç–æ–≥–æ–≤–æ–µ –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç–µ–π."},
                {"role": "user", "text": (
                    "–í–æ—Ç –∫—Ä–∞—Ç–∫–∏–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —á–∞—Å—Ç–µ–π –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:\n\n"
                    f"{merged}\n\n"
                    "–°–æ—Å—Ç–∞–≤—å –µ–¥–∏–Ω–æ–µ, —Å–≤—è–∑–Ω–æ–µ –∏ –∫—Ä–∞—Ç–∫–æ–µ –∏—Ç–æ–≥–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, –∏–∑–±–µ–≥–∞—è –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –Ω–µ—Å–æ—Å—Ç—ã–∫–æ–≤–æ–∫."
                )}
            ]
        result = (
            sdk.models.completions("yandexgpt").configure(temperature=0.5).run(messages)
        )
        print(result.alternatives[0].text)
        return result.alternatives[0].text
    
